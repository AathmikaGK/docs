---
title: Built-in middleware
description: Prebuilt middleware for common agent use cases
---

LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.

## Provider-agnostic middleware

These middleware work with any LLM provider.

| Middleware | Category | Use case |
|------------|----------|----------|
| [Summarization](#summarization) | Context management | Automatically summarize conversation history when approaching token limits |
| [Human-in-the-loop](#human-in-the-loop) | Control flow | Pause execution for human approval of tool calls |
| [Model call limit](#model-call-limit) | Control flow | Limit the number of model calls to prevent excessive costs |
| [Tool call limit](#tool-call-limit) | Control flow | Control tool execution by limiting call counts |
| [Model fallback](#model-fallback) | Reliability | Automatically fallback to alternative models when primary fails |
| [PII detection](#pii-detection) | Security & compliance | Detect and handle Personally Identifiable Information |
| [To-do list](#to-do-list) | Task planning | Equip agents with task planning and tracking capabilities |
| [LLM tool selector](#llm-tool-selector) | Tool management | Use an LLM to select relevant tools before calling main model |
| [Tool retry](#tool-retry) | Reliability | Automatically retry failed tool calls with exponential backoff |
| [LLM tool emulator](#llm-tool-emulator) | Tool management | Emulate tool execution using LLM for testing purposes |
| [Context editing](#context-editing) | Context management | Manage conversation context by trimming or clearing tool uses |

---

### Summarization

Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context.

<Tip>
**Perfect for:**
- Long-running conversations that exceed context windows
- Multi-turn dialogues with extensive history
- Applications where preserving full conversation context matters
</Tip>

**API reference:** @[`SummarizationMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger={"tokens": 4000},
            keep={"messages": 20},
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      trigger: { tokens: 4000 },
      keep: { messages: 20 },
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**`model`** (required) - LLM model to use for summarization

**`trigger`** (required) - When to trigger summarization. Can be a single condition object or array of conditions:
- `fraction` - Fraction of model's context size (0-1)
- `tokens` - Absolute token count
- `messages` - Number of messages

**`keep`** (required) - How much context to preserve after summarization (specify exactly one):
- `fraction` - Fraction of model's context size to keep
- `tokens` - Absolute token count to keep
- `messages` - Number of recent messages to keep

</Accordion>

<Accordion title="Full example">

The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.

**Trigger conditions** control when summarization runs:
- Single condition object (all properties must be met - AND logic)
- Array of conditions (any condition must be met - OR logic)
- Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)

**Keep conditions** control how much context to preserve (specify exactly one):
- `fraction` - Fraction of model's context size to keep
- `tokens` - Absolute token count to keep
- `messages` - Number of recent messages to keep

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware


# Single condition: trigger if tokens >= 4000 AND messages >= 10
agent = create_agent(
    model="gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger={"tokens": 4000, "messages": 10},
            keep={"messages": 20},
        ),
    ],
)

# Multiple conditions
agent2 = create_agent(
    model="gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger=[
                {"tokens": 5000, "messages": 3},
                {"tokens": 3000, "messages": 6},
            ],
            keep={"messages": 20},
        ),
    ],
)

# Using fractional limits
agent3 = create_agent(
    model="gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger={"fraction": 0.8},
            keep={"fraction": 0.3},
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, summarizationMiddleware } from "langchain";

// Single condition
const agent = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      trigger: { tokens: 4000, messages: 10 },
      keep: { messages: 20 },
    }),
  ],
});

// Multiple conditions
const agent2 = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      trigger: [
        { tokens: 5000, messages: 3 },
        { tokens: 3000, messages: 6 },
      ],
      keep: { messages: 20 },
    }),
  ],
});

// Using fractional limits
const agent3 = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      trigger: { fraction: 0.8 },
      keep: { fraction: 0.3 },
    }),
  ],
});
```
:::

</Accordion>

---

### Human-in-the-loop

Pause agent execution for human approval, editing, or rejection of tool calls before they execute.

<Tip>
**Perfect for:**
- High-stakes operations requiring human approval (database writes, financial transactions)
- Compliance workflows where human oversight is mandatory
- Long-running conversations where human feedback guides the agent
</Tip>

**API reference:** @[`HumanInTheLoopMiddleware`]

<Note>
    **Important:** Human-in-the-loop middleware requires a [checkpointer](/oss/langgraph/persistence#checkpoints) to maintain state across interruptions.

    See the [human-in-the-loop documentation](/oss/langchain/human-in-the-loop) for complete examples and integration patterns.
</Note>

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model="gpt-4o",
    tools=[read_email_tool, send_email_tool],
    checkpointer=InMemorySaver(),
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                "send_email_tool": {
                    "allowed_decisions": ["approve", "edit", "reject"],
                },
                "read_email_tool": False,
            }
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [readEmailTool, sendEmailTool],
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: {
        send_email: {
          allowAccept: true,
          allowEdit: true,
          allowRespond: true,
        },
        read_email: false,
      }
    })
  ]
});
```
:::

<Accordion title="Configuration options">

**`interrupt_on`** (required) - Dictionary mapping tool names to interrupt configuration:
- `false` - Never interrupt for this tool
- `true` - Interrupt with all decisions enabled
- Object with `allowed_decisions` array containing: `"approve"`, `"edit"`, `"reject"`

</Accordion>

<Accordion title="Full example">

See the [human-in-the-loop guide](/oss/langchain/human-in-the-loop) for comprehensive examples showing how to:
- Handle interruptions and resume execution
- Implement custom approval workflows
- Integrate with UI components
- Manage conversation state across interruptions

</Accordion>

---

### Model call limit

Limit the number of model calls to prevent infinite loops or excessive costs.

<Tip>
**Perfect for:**
- Preventing runaway agents from making too many API calls
- Enforcing cost controls on production deployments
- Testing agent behavior within specific call budgets
</Tip>

**API reference:** @[`ModelCallLimitMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ModelCallLimitMiddleware(
            thread_limit=10,
            run_limit=5,
            exit_behavior="end",
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, modelCallLimitMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    modelCallLimitMiddleware({
      threadLimit: 10,
      runLimit: 5,
      exitBehavior: "end",
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**`thread_limit`** (optional) - Maximum model calls across all runs in a conversation thread (requires checkpointer)

**`run_limit`** (optional) - Maximum model calls per single invocation (resets each turn)

**`exit_behavior`** (optional) - What to do when limit is reached:
- `"end"` - Gracefully terminate execution (default)
- `"error"` - Raise/throw exception

</Accordion>

<Accordion title="Full example">

The middleware tracks model calls across two scopes:
- **Thread limit** - Max calls across all runs in a conversation thread (requires checkpointer)
- **Run limit** - Max calls per single invocation (resets each turn)

Exit behaviors:
- `'end'` - Graceful termination (default)
- `'error'` - Raise/throw exception

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelCallLimitMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool],
    checkpointer=InMemorySaver(),
    middleware=[
        ModelCallLimitMiddleware(
            thread_limit=10,
            run_limit=5,
            exit_behavior="end",
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, modelCallLimitMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [
    modelCallLimitMiddleware({
      threadLimit: 10,
      runLimit: 5,
      exitBehavior: "end",
    }),
  ],
});
```
:::

</Accordion>

---

### Tool call limit

Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools.

<Tip>
**Perfect for:**
- Preventing excessive calls to expensive external APIs
- Limiting web searches or database queries
- Enforcing rate limits on specific tool usage
- Protecting against runaway agent loops
</Tip>

**API reference:** @[`ToolCallLimitMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool],
    middleware=[
        # Global limit
        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),
        # Tool-specific limit
        ToolCallLimitMiddleware(
            tool_name="search",
            thread_limit=5,
            run_limit=3,
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, toolCallLimitMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, databaseTool],
  middleware: [
    toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 }),
    toolCallLimitMiddleware({
      toolName: "search",
      threadLimit: 5,
      runLimit: 3,
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**`tool_name`** (optional) - Specific tool to limit. If not provided, limits apply globally across all tools

**`thread_limit`** (optional) - Maximum tool calls across all runs in a conversation thread (requires checkpointer)

**`run_limit`** (optional) - Maximum tool calls per single invocation (resets each turn)

**`exit_behavior`** (optional) - What to do when limit is reached:
- `"continue"` - Block exceeded calls with error message, agent continues (default)
- `"error"` - Raise/throw exception immediately
- `"end"` - Stop with ToolMessage + AI message (single-tool scenarios only)

</Accordion>

<Accordion title="Full example">

Specify limits with:
- **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)
- **Run limit** - Max calls per single invocation (resets each turn)

Exit behaviors:
- `'continue'` (default) - Block exceeded calls with error messages, agent continues
- `'error'` - Raise exception immediately
- `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolCallLimitMiddleware


global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)
search_limiter = ToolCallLimitMiddleware(tool_name="search", thread_limit=5, run_limit=3)
database_limiter = ToolCallLimitMiddleware(tool_name="query_database", thread_limit=10)
strict_limiter = ToolCallLimitMiddleware(tool_name="scrape_webpage", run_limit=2, exit_behavior="error")

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool, scraper_tool],
    middleware=[global_limiter, search_limiter, database_limiter, strict_limiter],
)
```
:::

:::js
```typescript
import { createAgent, toolCallLimitMiddleware } from "langchain";

const globalLimiter = toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 });
const searchLimiter = toolCallLimitMiddleware({ toolName: "search", threadLimit: 5, runLimit: 3 });
const databaseLimiter = toolCallLimitMiddleware({ toolName: "query_database", threadLimit: 10 });
const strictLimiter = toolCallLimitMiddleware({ toolName: "scrape_webpage", runLimit: 2, exitBehavior: "error" });

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, databaseTool, scraperTool],
  middleware: [globalLimiter, searchLimiter, databaseLimiter, strictLimiter],
});
```
:::

</Accordion>

---

### Model fallback

Automatically fallback to alternative models when the primary model fails.

<Tip>
**Perfect for:**
- Building resilient agents that handle model outages
- Cost optimization by falling back to cheaper models
- Provider redundancy across OpenAI, Anthropic, etc.
</Tip>

**API reference:** @[`ModelFallbackMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelFallbackMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ModelFallbackMiddleware(
            "gpt-4o-mini",
            "claude-3-5-sonnet-20241022",
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, modelFallbackMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    modelFallbackMiddleware(
      "gpt-4o-mini",
      "claude-3-5-sonnet-20241022"
    ),
  ],
});
```
:::

<Accordion title="Configuration options">

**`*models`** (required) - One or more fallback model names to try in order when the primary model fails

</Accordion>

<Accordion title="Full example">

The middleware tries fallback models in order when the primary model fails.

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ModelFallbackMiddleware


agent = create_agent(
    model="gpt-4o",  # Primary model
    tools=[search_tool, calculator_tool],
    middleware=[
        ModelFallbackMiddleware(
            "gpt-4o-mini",
            "claude-3-5-sonnet-20241022",
            "claude-3-haiku-20240307",
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, modelFallbackMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [
    modelFallbackMiddleware(
      "gpt-4o-mini",
      "claude-3-5-sonnet-20241022",
      "claude-3-haiku-20240307"
    ),
  ],
});
```
:::

</Accordion>

---

### PII detection

Detect and handle Personally Identifiable Information (PII) in conversations using configurable strategies.

<Tip>
**Perfect for:**
- Healthcare and financial applications with compliance requirements
- Customer service agents that need to sanitize logs
- Any application handling sensitive user data
</Tip>

**API reference:** @[`PIIMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        PIIMiddleware("credit_card", strategy="mask", apply_to_input=True),
    ],
)
```
:::

:::js
```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    piiRedactionMiddleware({
      piiType: "credit_card",
      strategy: "mask",
      applyToInput: true,
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**PII type** (required) - Either a built-in type or custom regex pattern:
- Built-in types: `"email"`, `"credit_card"`, `"ip"`, `"mac_address"`, `"url"`
- Custom: `detector` parameter with regex pattern

**`strategy`** (required) - How to handle detected PII:
- `"block"` - Raise exception when detected
- `"redact"` - Replace with `[REDACTED_TYPE]`
- `"mask"` - Partially mask (e.g., `****-****-****-1234`)
- `"hash"` - Replace with deterministic hash

**Application scope** (all optional, default `False`):
- `apply_to_input` - Check user messages before model call
- `apply_to_output` - Check AI messages after model call
- `apply_to_tool_results` - Check tool result messages after execution

</Accordion>

<Accordion title="Full example">

The middleware supports detecting built-in PII types (`email`, `credit_card`, `ip`, `mac_address`, `url`) or custom types with regex patterns.

**Detection strategies:**
- `'block'` - Raise exception when detected
- `'redact'` - Replace with `[REDACTED_TYPE]`
- `'mask'` - Partially mask (e.g., `****-****-****-1234`)
- `'hash'` - Replace with deterministic hash

**Application scope:**
- `apply_to_input` - Check user messages before model call
- `apply_to_output` - Check AI messages after model call
- `apply_to_tool_results` - Check tool result messages after execution

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware


agent = create_agent(
    model="gpt-4o",
    tools=[database_tool, email_tool],
    middleware=[
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        PIIMiddleware("credit_card", strategy="mask", apply_to_input=True),
        PIIMiddleware("api_key", detector=r"sk-[a-zA-Z0-9]{32}", strategy="block"),
        PIIMiddleware("ssn", detector=r"\d{3}-\d{2}-\d{4}", strategy="hash", apply_to_tool_results=True),
    ],
)
```
:::

:::js
```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [databaseTool, emailTool],
  middleware: [
    piiRedactionMiddleware({ piiType: "email", strategy: "redact", applyToInput: true }),
    piiRedactionMiddleware({ piiType: "credit_card", strategy: "mask", applyToInput: true }),
    piiRedactionMiddleware({ piiType: "api_key", detector: /sk-[a-zA-Z0-9]{32}/, strategy: "block" }),
    piiRedactionMiddleware({ piiType: "ssn", detector: /\d{3}-\d{2}-\d{4}/, strategy: "hash", applyToToolResults: true }),
  ],
});
```
:::

</Accordion>

---

### To-do list

Equip agents with task planning and tracking capabilities for complex multi-step tasks.

<Tip>
**Perfect for:**
- Complex multi-step tasks requiring coordination across multiple tools
- Long-running operations where progress visibility is important
</Tip>

<Note>
    This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.
</Note>

**API reference:** @[`TodoListMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[read_file, write_file, run_tests],
    middleware=[TodoListMiddleware()],
)
```
:::

:::js
```typescript
import { createAgent, todoListMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [readFile, writeFile, runTests],
  middleware: [todoListMiddleware()] as const,
});
```
:::

<Accordion title="Configuration options">

This middleware has no configuration options. It automatically provides the agent with a `write_todos` tool and system prompts for task planning.

</Accordion>

<Accordion title="Full example">

Just as humans are more effective when they write down and track tasks, agents benefit from structured task management to break down complex problems.

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool


@tool
def read_file(file_path: str) -> str:
    """Read contents of a file."""
    with open(file_path) as f:
        return f.read()


@tool
def write_file(file_path: str, content: str) -> str:
    """Write content to a file."""
    with open(file_path, 'w') as f:
        f.write(content)
    return f"Wrote {len(content)} characters to {file_path}"


agent = create_agent(
    model="gpt-4o",
    tools=[read_file, write_file],
    middleware=[TodoListMiddleware()],
)

result = agent.invoke({
    "messages": [HumanMessage("Refactor the authentication module")]
})

print(result["todos"])  # Track progress
```
:::

:::js
```typescript
import { createAgent, HumanMessage, todoListMiddleware, tool } from "langchain";
import * as z from "zod";

const readFile = tool(
  async ({ filePath }) => "file contents",
  {
    name: "read_file",
    description: "Read contents of a file",
    schema: z.object({ filePath: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [readFile],
  middleware: [todoListMiddleware()] as const,
});

const result = await agent.invoke({
  messages: [new HumanMessage("Refactor the authentication module")],
});

console.log(result.todos);
```
:::

</Accordion>

---

### LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model.

<Tip>
**Perfect for:**
- Agents with many tools (10+) where most aren't relevant per query
- Reducing token usage by filtering irrelevant tools
- Improving model focus and accuracy
</Tip>

**API reference:** @[`LLMToolSelectorMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[tool1, tool2, tool3, tool4, tool5, ...],
    middleware=[
        LLMToolSelectorMiddleware(
            model="gpt-4o-mini",
            max_tools=3,
            always_include=["search"],
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, llmToolSelectorMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [tool1, tool2, tool3, tool4, tool5, ...],
  middleware: [
    llmToolSelectorMiddleware({
      model: "gpt-4o-mini",
      maxTools: 3,
      alwaysInclude: ["search"],
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**`model`** (required) - LLM model to use for tool selection (typically a cheaper/faster model)

**`max_tools`** (optional) - Maximum number of tools to select (default: 5)

**`always_include`** (optional) - List of tool names to always include in the selection

</Accordion>

<Accordion title="Full example">

The middleware uses a (typically cheaper) LLM to analyze the user's query and select the most relevant subset of tools.

**Benefits:**
- **Shorter prompts** - Reduce complexity by exposing only relevant tools
- **Better accuracy** - Models choose correctly from fewer options
- **Cost savings** - Use cheaper model for selection

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware


agent = create_agent(
    model="gpt-4o",
    tools=[search_web, query_database, send_email, get_weather, ...],
    middleware=[
        LLMToolSelectorMiddleware(
            model="gpt-4o-mini",
            max_tools=3,
            always_include=["search_web"],
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, llmToolSelectorMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchWeb, queryDatabase, sendEmail, getWeather, ...],
  middleware: [
    llmToolSelectorMiddleware({
      model: "gpt-4o-mini",
      maxTools: 3,
      alwaysInclude: ["search_web"],
    }),
  ],
});
```
:::

</Accordion>

---

### Tool retry

Automatically retry failed tool calls with configurable exponential backoff.

<Tip>
**Perfect for:**
- Handling transient failures in external API calls
- Improving reliability of network-dependent tools
- Building resilient agents that gracefully handle temporary errors
</Tip>

**API reference:** @[`ToolRetryMiddleware`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,
            initial_delay=1.0,
        ),
    ],
)
```
:::

:::js
This middleware is only available in Python. For JavaScript/TypeScript, consider implementing retry logic in your tool definitions or using a wrap-style middleware.
:::

<Accordion title="Configuration options">

**`max_retries`** (optional) - Number of retry attempts (default: 2)

**`backoff_factor`** (optional) - Multiplier for exponential backoff (default: 2.0)

**`initial_delay`** (optional) - Starting delay in seconds (default: 1.0)

**`max_delay`** (optional) - Maximum delay between retries in seconds (default: 60.0)

**`jitter`** (optional) - Add random variation to delays (default: True)

**`tools`** (optional) - List of specific tool names to retry. If not provided, retries all tools

**`retry_on`** (optional) - Tuple of exception types to retry on. If not provided, retries on all exceptions

**`on_failure`** (optional) - What to do after all retries fail:
- `"return_message"` - Return error message (default)
- `"raise"` - Re-raise the exception
- Custom callable - Function that returns an error message

</Accordion>

<Accordion title="Full example">

The middleware automatically retries failed tool calls with exponential backoff.

**Key configuration:**
- `max_retries` - Number of retry attempts (default: 2)
- `backoff_factor` - Multiplier for exponential backoff (default: 2.0)
- `initial_delay` - Starting delay in seconds (default: 1.0)
- `max_delay` - Cap on delay growth (default: 60.0)
- `jitter` - Add random variation (default: True)

**Failure handling:**
- `on_failure='return_message'` - Return error message
- `on_failure='raise'` - Re-raise exception
- Custom callable - Function returning error message

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware


agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool, api_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,
            initial_delay=1.0,
            max_delay=60.0,
            jitter=True,
            tools=["api_tool"],
            retry_on=(ConnectionError, TimeoutError),
            on_failure="return_message",
        ),
    ],
)
```
:::

</Accordion>

---

### LLM tool emulator

Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses.

<Tip>
**Perfect for:**
- Testing agent behavior without executing real tools
- Developing agents when external tools are unavailable or expensive
- Prototyping agent workflows before implementing actual tools
</Tip>

**API reference:** @[`LLMToolEmulator`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator

agent = create_agent(
    model="gpt-4o",
    tools=[get_weather, search_database, send_email],
    middleware=[
        LLMToolEmulator(),  # Emulate all tools
    ],
)
```
:::

:::js
This middleware is only available in Python. For JavaScript/TypeScript, consider creating mock tool implementations for testing.
:::

<Accordion title="Configuration options">

**`model`** (optional) - LLM model to use for emulation (default: uses agent's model)

**`tools`** (optional) - List of specific tool names to emulate. If not provided, emulates all tools

</Accordion>

<Accordion title="Full example">

The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator
from langchain_core.tools import tool


@tool
def get_weather(location: str) -> str:
    """Get the current weather for a location."""
    return f"Weather in {location}"

@tool
def send_email(to: str, subject: str, body: str) -> str:
    """Send an email."""
    return "Email sent"


# Emulate all tools
agent = create_agent(
    model="gpt-4o",
    tools=[get_weather, send_email],
    middleware=[LLMToolEmulator()],
)

# Emulate specific tools only
agent2 = create_agent(
    model="gpt-4o",
    tools=[get_weather, send_email],
    middleware=[LLMToolEmulator(tools=["get_weather"])],
)

# Use custom model
agent3 = create_agent(
    model="gpt-4o",
    tools=[get_weather, send_email],
    middleware=[LLMToolEmulator(model="claude-sonnet-4-5-20250929")],
)
```
:::

</Accordion>

---

### Context editing

Manage conversation context by trimming, summarizing, or clearing tool uses.

<Tip>
**Perfect for:**
- Long conversations that need periodic context cleanup
- Removing failed tool attempts from context
- Custom context management strategies
</Tip>

**API reference:** @[`ContextEditingMiddleware`], @[`ClearToolUsesEdit`]

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ContextEditingMiddleware(
            edits=[
                ClearToolUsesEdit(
                    trigger=100000,
                    keep=3,
                ),
            ],
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    contextEditingMiddleware({
      edits: [
        new ClearToolUsesEdit({
          triggerTokens: 100000,
          keep: 3,
        }),
      ],
    }),
  ],
});
```
:::

<Accordion title="Configuration options">

**`edits`** (required) - List of edit strategies to apply. Most commonly uses `ClearToolUsesEdit`:

**ClearToolUsesEdit parameters:**
- `trigger` - Token count threshold to trigger clearing
- `keep` - Number of most recent tool results to preserve
- `clear_tool_inputs` (optional) - Whether to clear tool call arguments (default: False)
- `exclude_tools` (optional) - List of tool names to never clear
- `placeholder` (optional) - Text to replace cleared content (default: "[cleared]")

</Accordion>

<Accordion title="Full example">

The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.

**How it works:**
1. Monitor token count in conversation
2. When threshold is reached, clear older tool outputs
3. Keep most recent N tool results
4. Optionally preserve tool call arguments for context

:::python
```python
from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit


agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool, database_tool],
    middleware=[
        ContextEditingMiddleware(
            edits=[
                ClearToolUsesEdit(
                    trigger=2000,
                    keep=3,
                    clear_tool_inputs=False,
                    exclude_tools=[],
                    placeholder="[cleared]",
                ),
            ],
        ),
    ],
)
```
:::

:::js
```typescript
import { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool, databaseTool],
  middleware: [
    contextEditingMiddleware({
      edits: [
        new ClearToolUsesEdit({
          triggerTokens: 2000,
          keep: 3,
          clearToolInputs: false,
          excludeTools: [],
          placeholder: "[cleared]",
        }),
      ],
    }),
  ],
});
```
:::

</Accordion>

---

## Provider-specific middleware

These middleware are optimized for specific LLM providers.

### Anthropic

Middleware specifically designed for Anthropic's Claude models.

| Middleware | Category | Use case |
|------------|----------|----------|
| [Prompt caching](#anthropic-prompt-caching) | Context management | Reduce costs by caching repetitive prompt prefixes |

---

#### Anthropic prompt caching

Reduce costs by caching repetitive prompt prefixes with Anthropic models.

<Tip>
**Perfect for:**
- Applications with long, repeated system prompts
- Agents that reuse the same context across invocations
- Reducing API costs for high-volume deployments
</Tip>

<Info>
    Learn more about [Anthropic Prompt Caching](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#cache-limitations) strategies and limitations.
</Info>

**API reference:** @[`AnthropicPromptCachingMiddleware`]

:::python
```python
from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
    system_prompt="<Your long system prompt here>",
    middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)
```
:::

:::js
```typescript
import { createAgent, anthropicPromptCachingMiddleware } from "langchain";

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  prompt: "<Your long system prompt here>",
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});
```
:::

<Accordion title="Configuration options">

**`ttl`** (optional) - Time-to-live for cached prompts. Accepts duration strings like `"5m"`, `"1h"`, etc. (default: `"5m"`)

</Accordion>

<Accordion title="Full example">

:::python
```python
from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage


LONG_PROMPT = """
Please be a helpful assistant.

<Lots more context ...>
"""

agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
    system_prompt=LONG_PROMPT,
    middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)

# cache store
agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

# cache hit, system prompt is cached
agent.invoke({"messages": [HumanMessage("What's my name?")]})
```
:::

:::js
```typescript
import { createAgent, HumanMessage, anthropicPromptCachingMiddleware } from "langchain";

const LONG_PROMPT = `
Please be a helpful assistant.

<Lots more context ...>
`;

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  prompt: LONG_PROMPT,
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});

// cache store
await agent.invoke({
  messages: [new HumanMessage("Hi, my name is Bob")]
});

// cache hit
const result = await agent.invoke({
  messages: [new HumanMessage("What's my name?")]
});
```
:::

</Accordion>

---

### OpenAI

Middleware specifically designed for OpenAI models.

<Note>
Coming soon! Check back for OpenAI-specific middleware optimizations.
</Note>
